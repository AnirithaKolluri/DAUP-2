{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dPGXx3hFCCOk",
    "outputId": "47e89dec-2930-4f50-900d-ad0fffcd774e"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "dataset_path = kagglehub.dataset_download(\"kritikseth/fruit-and-vegetable-image-recognition\")\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    print(\"Contents of the dataset folder:\")\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        print(f\"Directory: {root}\")\n",
    "        for file in files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Find image files in the dataset path (including subdirectories)\n",
    "image_files = []\n",
    "for root, _, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "\n",
    "# Select a subset of 40â€“50 images\n",
    "num_images_to_use = random.randint(40, 50)\n",
    "selected_images = random.sample(image_files, min(num_images_to_use, len(image_files)))\n",
    "\n",
    "# Optional: Copy selected images to a new folder for easier management\n",
    "output_folder = \"selected_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "for img_path in selected_images:\n",
    "    img_filename = os.path.basename(img_path)\n",
    "    shutil.copy(img_path, os.path.join(output_folder, img_filename))\n",
    "\n",
    "# Load and preprocess the selected images\n",
    "image_data = []\n",
    "for img_path in selected_images:\n",
    "    try:\n",
    "        img = Image.open(img_path).resize((224, 224))  # Resize to 224x224\n",
    "        img_array = np.array(img) / 255.0  # Normalize pixel values\n",
    "        image_data.append(img_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(image_data)} images for training from {dataset_path}.\")\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to convert image to grayscale and RGB\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return gray_img, rgb_img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Use selected_images instead of image_data\n",
    "for img_path in selected_images:  # Iterate through the list of image paths\n",
    "    gray, rgb = process_image(img_path)\n",
    "\n",
    "    if gray is not None and rgb is not None:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(gray, cmap='gray')\n",
    "        plt.title(\"Grayscale\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(\"RGB\")\n",
    "        plt.show()\n",
    "\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize to 200x200\n",
    "        img_200 = cv2.resize(img_rgb, (200, 200))\n",
    "\n",
    "        # Resize to 256x256\n",
    "        img_256 = cv2.resize(img_rgb, (256, 256))\n",
    "\n",
    "        return img_200, img_256\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Iterate directly through selected_images (list of file paths)\n",
    "for image_path in selected_images:\n",
    "    img_200, img_256 = process_image(image_path)\n",
    "\n",
    "    if img_200 is not None and img_256 is not None:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_200)\n",
    "        plt.title(\"200x200 RGB\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img_256)\n",
    "        plt.title(\"256x256 RGB\")\n",
    "        plt.show()\n",
    "\n",
    "# Define dataset split (33% train, 33% val, 33% test)\n",
    "split_ratio = [0.34, 0.33, 0.33]\n",
    "train_split = int(split_ratio[0] * len(selected_images))\n",
    "val_split = int(split_ratio[1] * len(selected_images))\n",
    "\n",
    "train_images = selected_images[:train_split]\n",
    "val_images = selected_images[train_split:train_split + val_split]\n",
    "test_images = selected_images[train_split + val_split:]\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = {\n",
    "    \"train\": \"dataset/train\",\n",
    "    \"val\": \"dataset/val\",\n",
    "    \"test\": \"dataset/test\"\n",
    "}\n",
    "\n",
    "for _, path in output_dirs.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Function to copy images to respective directories\n",
    "def copy_images(image_list, destination_folder):\n",
    "    for img_path in image_list:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        shutil.copy(img_path, os.path.join(destination_folder, img_filename))\n",
    "\n",
    "copy_images(train_images, output_dirs[\"train\"])\n",
    "copy_images(val_images, output_dirs[\"val\"])\n",
    "copy_images(test_images, output_dirs[\"test\"])\n",
    "\n",
    "print(f\"Dataset spliting completed! \\nTrain: {len(train_images)} images\\nValidation: {len(val_images)} images\\nTest: {len(test_images)} images.\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train_dir = \"dataset/train\"\n",
    "test_dir = \"dataset/test\"\n",
    "\n",
    "# Create class folders (Modify class names based on your dataset)\n",
    "os.makedirs(os.path.join(train_dir, \"class1\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"class2\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(test_dir, \"class1\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(test_dir, \"class2\"), exist_ok=True)\n",
    "\n",
    "# Move images into class folders (Here, randomly distributing them)\n",
    "for img in os.listdir(train_dir):\n",
    "    if img.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        src_path = os.path.join(train_dir, img)\n",
    "        # Extract the numeric part of the filename using a regular expression\n",
    "        import re\n",
    "        numeric_part = re.search(r'\\d+', img).group()\n",
    "        # Use the numeric part for class assignment\n",
    "        dst_path = os.path.join(train_dir, \"class1\" if int(numeric_part) % 2 == 0 else \"class2\", img)\n",
    "        shutil.move(src_path, dst_path)\n",
    "\n",
    "for img in os.listdir(test_dir):\n",
    "    if img.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        src_path = os.path.join(test_dir, img)\n",
    "        # Extract the numeric part of the filename using a regular expression\n",
    "        import re\n",
    "        numeric_part = re.search(r'\\d+', img).group()\n",
    "        # Use the numeric part for class assignment\n",
    "        dst_path = os.path.join(test_dir, \"class1\" if int(numeric_part) % 2 == 0 else \"class2\", img)\n",
    "        shutil.move(src_path, dst_path)\n",
    "\n",
    "print(\"Dataset structure fixed!\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = (200, 200)  # Change to (256,256) if needed\n",
    "COLOR_MODE = \"rgb\"  # Change to \"grayscale\" if needed\n",
    "INPUT_SHAPE = (200, 200, 3) if COLOR_MODE == \"rgb\" else (200, 200, 1)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Data augmentation and loading\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"dataset/train\",  # Verify this path is correct\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=COLOR_MODE,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    \"dataset/train\",  # Verify this path is correct\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=COLOR_MODE,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    \"dataset/test\",  # Verify this path is correct\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=COLOR_MODE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=INPUT_SHAPE),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"car_crash_cnn_model.h5\")\n",
    "\n",
    "# Import the scipy.stats module\n",
    "from scipy import stats\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = (200, 200)  # Change to (256,256) if needed\n",
    "COLOR_MODE = \"rgb\"  # Change to \"grayscale\" if needed\n",
    "INPUT_SHAPE = (200, 200, 3) if COLOR_MODE == \"rgb\" else (200, 200, 1)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "train_accuracies = history.history['accuracy']\n",
    "val_accuracies = history.history['val_accuracy']\n",
    "\n",
    "train_losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "\n",
    "train_accuracies = np.array(train_accuracies)\n",
    "val_accuracies = np.array(val_accuracies)\n",
    "\n",
    "# Perform Z-test (using ttest_ind)\n",
    "z_score, p_value_z = stats.ttest_ind(train_accuracies, val_accuracies, equal_var=True)\n",
    "print(f\"\\nZ-test Results:\\nZ-score = {z_score}\\nP-value = {p_value_z}\")\n",
    "\n",
    "# Perform T-test (assuming small sample size or unknown variances)\n",
    "t_score, p_value_t = stats.ttest_ind(train_accuracies, val_accuracies, equal_var=False)\n",
    "print(f\"\\nT-test Results:\\nT-score = {t_score}\\nP-value = {p_value_t}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_probs = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# True labels\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Optional: Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = list(test_generator.class_indices.keys())\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# Optional: Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names,\n",
    "            yticklabels=target_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
